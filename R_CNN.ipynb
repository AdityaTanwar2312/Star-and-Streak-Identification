{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ece620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from torchvision.ops import box_iou\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42db38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full annotations CSV\n",
    "df = pd.read_csv(\"annotation_data/annotations.csv\")\n",
    "\n",
    "# Get unique image filenames\n",
    "unique_images = df[\"image\"].unique()\n",
    "\n",
    "# Split into train/test image filenames\n",
    "train_imgs, test_imgs = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df[df[\"image\"].isin(train_imgs)].reset_index(drop=True)\n",
    "test_df = df[df[\"image\"].isin(test_imgs)].reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(\"annotation_data/train_annotations.csv\", index=False)\n",
    "test_df.to_csv(\"annotation_data/test_annotations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04674910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(file_list, src_dir, dest_dir):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    for fname in file_list:\n",
    "        src = os.path.join(src_dir, fname)\n",
    "        dest = os.path.join(dest_dir, fname)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest)\n",
    "\n",
    "copy_images(train_imgs, \"Datasets/enhanced_images\", \"Datasets/train_images\")\n",
    "copy_images(test_imgs, \"Datasets/enhanced_images\", \"Datasets/test_images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbc5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StarStreakDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transforms=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Map labels to integers\n",
    "        self.label_map = {'star': 1, 'streak': 2}  # Add more if needed\n",
    "\n",
    "        # Group annotations by image\n",
    "        self.image_ids = self.df['image'].unique()\n",
    "        self.image_data = self.df.groupby('image')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        records = self.image_data.get_group(img_id)\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, img_id)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for _, row in records.iterrows():\n",
    "            xmin = row['x_min']\n",
    "            ymin = row['y_min']\n",
    "            xmax = row['x_max']\n",
    "            ymax = row['y_max']\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            label = row['label']\n",
    "            if isinstance(label, str):  # Convert string label to integer\n",
    "                label = self.label_map[label.lower()]\n",
    "            labels.append(label)\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7054d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StarStreakDataset(\n",
    "    csv_file=\"annotation_data/train_annotations.csv\",\n",
    "    image_dir=\"Datasets/train_images\",\n",
    "    transforms=ToTensor()\n",
    ")\n",
    "\n",
    "test_dataset = StarStreakDataset(\n",
    "    csv_file=\"annotation_data/test_annotations.csv\",\n",
    "    image_dir=\"Datasets/test_images\",\n",
    "    transforms=ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c8f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[143., 101., 154., 113.],\n",
      "        [146., 197., 160., 203.],\n",
      "        [346., 381., 357., 393.],\n",
      "        [521., 469., 533., 480.]]), 'labels': tensor([1, 2, 1, 1]), 'image_id': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "sample_img, sample_target = train_dataset[0]\n",
    "print(sample_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f09826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NARINDER\\Desktop\\Intern_Assignments\\Digantara\\segment\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NARINDER\\Desktop\\Intern_Assignments\\Digantara\\segment\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0 [Train]: 100%|██████████| 28/28 [02:19<00:00,  4.99s/it, loss=0.929]\n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.77s/it, loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0, Train Loss: 1.2452, Validation Loss: 0.3800\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 28/28 [02:16<00:00,  4.89s/it, loss=0.296]\n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.83s/it, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Train Loss: 0.4949, Validation Loss: 0.2601\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 28/28 [02:16<00:00,  4.89s/it, loss=0.145]\n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.82s/it, loss=0.0904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2, Train Loss: 0.2788, Validation Loss: 0.0914\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 28/28 [02:17<00:00,  4.92s/it, loss=0.351] \n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.84s/it, loss=0.225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3, Train Loss: 0.2463, Validation Loss: 0.2229\n",
      " No improvement. Patience: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 28/28 [02:17<00:00,  4.92s/it, loss=0.176]\n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.80s/it, loss=0.276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4, Train Loss: 0.2281, Validation Loss: 0.2780\n",
      " No improvement. Patience: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 28/28 [02:16<00:00,  4.86s/it, loss=0.16] \n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.78s/it, loss=0.0984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5, Train Loss: 0.2306, Validation Loss: 0.1026\n",
      " No improvement. Patience: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 28/28 [02:15<00:00,  4.85s/it, loss=0.116] \n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.82s/it, loss=0.0581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6, Train Loss: 0.1449, Validation Loss: 0.0664\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 28/28 [02:15<00:00,  4.85s/it, loss=0.0876]\n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.81s/it, loss=0.0651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7, Train Loss: 0.1205, Validation Loss: 0.0678\n",
      " No improvement. Patience: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 28/28 [02:16<00:00,  4.87s/it, loss=0.0506]\n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.79s/it, loss=0.0621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8, Train Loss: 0.1157, Validation Loss: 0.0585\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 28/28 [02:19<00:00,  4.97s/it, loss=0.0785]\n",
      "Validation: 100%|██████████| 7/7 [00:13<00:00,  1.87s/it, loss=0.0448]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9, Train Loss: 0.1025, Validation Loss: 0.0558\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Setup\n",
    "def get_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch} [Train]\")\n",
    "\n",
    "    for i, (images, targets) in progress_bar:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        if i % print_freq == 0:\n",
    "            progress_bar.set_postfix(loss=losses.item())\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.train()  # set to train mode to get loss from model\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Validation\")\n",
    "\n",
    "    with torch.no_grad():  # prevent gradient computation\n",
    "        for i, (images, targets) in progress_bar:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "            progress_bar.set_postfix(loss=losses.item())\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, test_loader, device, num_epochs=10, checkpoint_path='model_output/model_checkpoint.pth'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "    writer = SummaryWriter()\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 5\n",
    "    log_df = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "        val_loss = evaluate(model, test_loader, device)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        log_df.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
    "        pd.DataFrame(log_df).to_csv(\"training_log.csv\", index=False)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\" Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" No improvement. Patience: {patience_counter}/{early_stop_patience}\")\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(\" Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = get_model(num_classes=3)  # background, star, streak\n",
    "model.to(device)\n",
    "\n",
    "train_model(model, train_loader, test_loader, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428fa82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Objects with Centroid Coordinates:\n",
      "Star (Score: 0.99) -> Centroid: (x=351.51, y=387.08)\n",
      "Star (Score: 0.98) -> Centroid: (x=148.30, y=106.74)\n",
      "Star (Score: 0.98) -> Centroid: (x=527.03, y=474.39)\n",
      "Streak (Score: 0.98) -> Centroid: (x=153.10, y=200.05)\n",
      "Star (Score: 0.65) -> Centroid: (x=441.33, y=521.56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NARINDER\\Desktop\\Intern_Assignments\\Digantara\\segment\\Lib\\site-packages\\torchvision\\utils.py:233: UserWarning: Argument 'font_size' will be ignored since 'font' is not set.\n",
      "  warnings.warn(\"Argument 'font_size' will be ignored since 'font' is not set.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define label map\n",
    "label_map = {1: 'Star', 2: 'Streak'}\n",
    "\n",
    "# Transformation\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image), image\n",
    "\n",
    "def get_centroids(boxes):\n",
    "    \"\"\"Compute centroids from bounding boxes.\"\"\"\n",
    "    centroids = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        centroids.append((cx.item(), cy.item()))\n",
    "    return centroids\n",
    "\n",
    "def predict_centroids(model, image_path, device, threshold=0.5, visualize=True):\n",
    "    model.eval()\n",
    "    image_tensor, orig_image = load_image(image_path)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor])[0]\n",
    "\n",
    "    boxes = prediction['boxes']\n",
    "    labels = prediction['labels']\n",
    "    scores = prediction['scores']\n",
    "\n",
    "    # Filter predictions\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "\n",
    "    # Calculate centroids\n",
    "    centroids = get_centroids(boxes)\n",
    "\n",
    "    # Print centroids with labels\n",
    "    print(\"Detected Objects with Centroid Coordinates:\")\n",
    "    for i, (label, score, centroid) in enumerate(zip(labels, scores, centroids)):\n",
    "        label_str = label_map.get(label.item(), f\"Class {label.item()}\")\n",
    "        print(f\"{label_str} (Score: {score.item():.2f}) -> Centroid: (x={centroid[0]:.2f}, y={centroid[1]:.2f})\")\n",
    "\n",
    "    # Optional: Visualize\n",
    "    if visualize:\n",
    "        label_texts = [f\"{label_map[l.item()]}: {s.item():.2f}\" for l, s in zip(labels, scores)]\n",
    "        drawn_image = draw_bounding_boxes(\n",
    "            (image_tensor * 255).byte().cpu(),\n",
    "            boxes=boxes.cpu(),\n",
    "            labels=label_texts,\n",
    "            colors=\"green\",\n",
    "            width=2,\n",
    "            font_size=20\n",
    "        )\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(drawn_image.permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Predictions with Centroids\")\n",
    "        plt.show()\n",
    "\n",
    "    return centroids\n",
    "\n",
    "# Usage\n",
    "image_path = \"Datasets/train_images/Raw_Observation_001_Set1_enhanced_plot.png\"\n",
    "model.load_state_dict(torch.load(\"model_output/model_checkpoint.pth\", map_location=device))\n",
    "centroids = predict_centroids(model, image_path, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e40b3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detection_accuracy(model, data_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"Evaluating Accuracy\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        for pred, target in zip(outputs, targets):\n",
    "            pred_boxes = pred['boxes']\n",
    "            pred_labels = pred['labels']\n",
    "            pred_scores = pred['scores']\n",
    "\n",
    "            true_boxes = target['boxes']\n",
    "            true_labels = target['labels']\n",
    "\n",
    "            if len(pred_boxes) == 0 or len(true_boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            ious = box_iou(pred_boxes, true_boxes)\n",
    "\n",
    "            for i, (box, label) in enumerate(zip(pred_boxes, pred_labels)):\n",
    "                max_iou, max_idx = ious[i].max(0)\n",
    "                if max_iou >= iou_threshold and label == true_labels[max_idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"Detection Accuracy: {accuracy*100:.2f}% ({correct}/{total})\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c55631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|██████████| 28/28 [00:52<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Accuracy: 61.86% (193/312)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|██████████| 7/7 [00:12<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Accuracy: 66.67% (42/63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_acc = compute_detection_accuracy(model, train_loader, device)\n",
    "test_acc = compute_detection_accuracy(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87b3358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-3): 4 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87402c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
