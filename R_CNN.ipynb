{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ece620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from torchvision.ops import box_iou\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full annotations CSV\n",
    "df = pd.read_csv(\"annotation_data/annotations.csv\")\n",
    "\n",
    "# Get unique image filenames\n",
    "unique_images = df[\"image\"].unique()\n",
    "\n",
    "# Split into train/test image filenames\n",
    "train_imgs, test_imgs = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df[df[\"image\"].isin(train_imgs)].reset_index(drop=True)\n",
    "test_df = df[df[\"image\"].isin(test_imgs)].reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(\"annotation_data/train_annotations.csv\", index=False)\n",
    "test_df.to_csv(\"annotation_data/test_annotations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04674910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(file_list, src_dir, dest_dir):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    for fname in file_list:\n",
    "        src = os.path.join(src_dir, fname)\n",
    "        dest = os.path.join(dest_dir, fname)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest)\n",
    "\n",
    "copy_images(train_imgs, \"Datasets/enhanced_images\", \"Datasets/train_images\")\n",
    "copy_images(test_imgs, \"Datasets/enhanced_images\", \"Datasets/test_images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbc5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StarStreakDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transforms=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Map labels to integers\n",
    "        self.label_map = {'star': 1, 'streak': 2}  # Add more if needed\n",
    "\n",
    "        # Group annotations by image\n",
    "        self.image_ids = self.df['image'].unique()\n",
    "        self.image_data = self.df.groupby('image')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        records = self.image_data.get_group(img_id)\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, img_id)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for _, row in records.iterrows():\n",
    "            xmin = row['x_min']\n",
    "            ymin = row['y_min']\n",
    "            xmax = row['x_max']\n",
    "            ymax = row['y_max']\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            label = row['label']\n",
    "            if isinstance(label, str):  # Convert string label to integer\n",
    "                label = self.label_map[label.lower()]\n",
    "            labels.append(label)\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StarStreakDataset(\n",
    "    csv_file=\"annotation_data/train_annotations.csv\",\n",
    "    image_dir=\"Datasets/train_images\",\n",
    "    transforms=ToTensor()\n",
    ")\n",
    "\n",
    "test_dataset = StarStreakDataset(\n",
    "    csv_file=\"annotation_data/test_annotations.csv\",\n",
    "    image_dir=\"Datasets/test_images\",\n",
    "    transforms=ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c8f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[143., 101., 154., 113.],\n",
      "        [146., 197., 160., 203.],\n",
      "        [346., 381., 357., 393.],\n",
      "        [521., 469., 533., 480.]]), 'labels': tensor([1, 2, 1, 1]), 'image_id': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "sample_img, sample_target = train_dataset[0]\n",
    "print(sample_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f09826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NARINDER\\Desktop\\Intern_Assignments\\Digantara\\segment\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NARINDER\\Desktop\\Intern_Assignments\\Digantara\\segment\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 0 [Train]: 100%|██████████| 28/28 [02:39<00:00,  5.68s/it, loss=0.4]  \n",
      "Validation: 100%|██████████| 7/7 [00:13<00:00,  1.92s/it, loss=0.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0, Train Loss: 1.1888, Validation Loss: 0.4216\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 28/28 [02:28<00:00,  5.29s/it, loss=0.482]\n",
      "Validation: 100%|██████████| 7/7 [00:13<00:00,  1.88s/it, loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Train Loss: 0.4794, Validation Loss: 0.2501\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 28/28 [02:28<00:00,  5.30s/it, loss=0.206]\n",
      "Validation: 100%|██████████| 7/7 [00:13<00:00,  1.92s/it, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2, Train Loss: 0.3352, Validation Loss: 0.1872\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 28/28 [02:27<00:00,  5.27s/it, loss=1.27] \n",
      "Validation: 100%|██████████| 7/7 [00:14<00:00,  2.02s/it, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3, Train Loss: 0.2284, Validation Loss: 0.1456\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 28/28 [02:30<00:00,  5.36s/it, loss=0.23] \n",
      "Validation: 100%|██████████| 7/7 [00:13<00:00,  1.93s/it, loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4, Train Loss: 0.2155, Validation Loss: 0.1167\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 28/28 [02:28<00:00,  5.32s/it, loss=0.16] \n",
      "Validation: 100%|██████████| 7/7 [00:13<00:00,  1.93s/it, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5, Train Loss: 0.2121, Validation Loss: 0.1406\n",
      " No improvement. Patience: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 28/28 [02:34<00:00,  5.51s/it, loss=0.578]\n",
      "Validation: 100%|██████████| 7/7 [00:13<00:00,  1.95s/it, loss=0.0769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6, Train Loss: 0.1689, Validation Loss: 0.0862\n",
      " Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 28/28 [02:29<00:00,  5.36s/it, loss=0.0711]\n",
      "Validation: 100%|██████████| 7/7 [00:14<00:00,  2.04s/it, loss=0.076] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7, Train Loss: 0.1314, Validation Loss: 0.0890\n",
      " No improvement. Patience: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 28/28 [02:25<00:00,  5.20s/it, loss=0.213] \n",
      "Validation: 100%|██████████| 7/7 [00:12<00:00,  1.85s/it, loss=0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8, Train Loss: 0.1907, Validation Loss: 0.1520\n",
      " No improvement. Patience: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 28/28 [02:27<00:00,  5.28s/it, loss=0.0863]\n",
      "Validation: 100%|██████████| 7/7 [00:13<00:00,  1.88s/it, loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9, Train Loss: 0.1768, Validation Loss: 0.1425\n",
      " No improvement. Patience: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Setup\n",
    "def get_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch} [Train]\")\n",
    "\n",
    "    for i, (images, targets) in progress_bar:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        if i % print_freq == 0:\n",
    "            progress_bar.set_postfix(loss=losses.item())\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.train()  # set to train mode to get loss from model\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Validation\")\n",
    "\n",
    "    with torch.no_grad():  # prevent gradient computation\n",
    "        for i, (images, targets) in progress_bar:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "            progress_bar.set_postfix(loss=losses.item())\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, test_loader, device, num_epochs=10, checkpoint_path='model_output/model_checkpoint.pth'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "    writer = SummaryWriter()\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 5\n",
    "    log_df = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "        val_loss = evaluate(model, test_loader, device)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        log_df.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
    "        pd.DataFrame(log_df).to_csv(\"training_log.csv\", index=False)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\" Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" No improvement. Patience: {patience_counter}/{early_stop_patience}\")\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(\" Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = get_model(num_classes=3)  # background, star, streak\n",
    "model.to(device)\n",
    "\n",
    "train_model(model, train_loader, test_loader, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fa82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Objects with Centroid Coordinates:\n",
      "Star (Score: 0.99) -> Centroid: (x=351.50, y=387.20)\n",
      "Star (Score: 0.97) -> Centroid: (x=148.10, y=106.87)\n",
      "Star (Score: 0.96) -> Centroid: (x=526.77, y=474.30)\n",
      "Streak (Score: 0.94) -> Centroid: (x=153.10, y=199.76)\n",
      "Star (Score: 0.85) -> Centroid: (x=441.34, y=521.68)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAMsCAYAAADJXzRsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAN2BJREFUeJzt3QeUbWdZP/7nTLslvZOEJISQ0AkQegdBqvwREAVlAWJDBbHxZ6lLpNk7gkvUpf5dKP4sNJUiijSlhCAYSoAU0kgj/Sa3zez/es999v2dezL33rl5Zu60z4d1mJl99tnnPWfOzbzf/bzvuwdd13UBAABQMFF5MAAAQCNYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgA69rd7na3eOlLX7r75//8z/+MwWAw/LpY2vF+5Vd+JdaKSy65ZPia/vIv/3LB+/72b//2QWnbav/87U17r9v72N5PgJVKsACWTd9Z6m8bN26Ms846K37yJ38yrr766lhN/vVf/3VNhYeV9vrb5+Hnfu7n4l73ulds3rw5DjnkkDjnnHPiTW96U9x4441L9ry33Xbb8HUtZtAEWKumlrsBAG94wxvi9NNPj61bt8YnPvGJ+OM//uNhR/X8888fdiIPpsc97nFx++23x8zMzAE9rrX3rW9967yd63a8qam185/b0047bfiapqenF/T6qz772c/GM57xjLj11lvjB37gB4aBojn33HPj13/91+NjH/tYfOhDH4qlChavf/3rh98/4QlPWPTjX3DBBTEx4RwfsDasnb90wKr19Kc/PR7ykIcMv/+hH/qhOOaYY+J3f/d34z3veU+88IUvnPcxW7ZsGZ61Xmytk9cqJ4tpsY+33Prq0sHQqhHf/d3fHZOTk/H5z39+WLEY9eY3vzn+9E//NFaKA/1cbtiwYUnbA3AwOU0CrDhPetKThl8vvvji4dc2Bv3QQw+NCy+8cHjm+rDDDovv//7vH943NzcXv//7vx/3ve99h53dE044IX70R380brjhhj2O2XXdcNjMXe9612EV5IlPfGJ86UtfusNz722Oxac//enhcx911FHDjuMDHvCA+IM/+IPd7Wtn65vRoV37mmPROsktUB1++OHD1/Yd3/Ed8alPfWreoWKf/OQn42d+5mfiuOOOGz5362hfe+21e+zbzt4/9alPjWOPPTY2bdo0rAD94A/+4D7f53bMFuLae9N75StfOXzOP/zDP9xjGFLb1ipJ882x2N/r77397W+PM844Y9iZfuhDHzqsROzPn/zJn8QVV1wxDJrjoaJpv+9f+qVf2mPb+9///njsYx87fK/aZ+WZz3zmHX7X/WeqHfs5z3nO8Pv2/rbhVrOzs7tfZ9vWtKpF/7r63+W+PpctYPzsz/5snHLKKcPXe8973nM4z2T0vd7bHIvW1vZvoP0e2+e1fW7b53zcnfmdAywlFQtgxWkdtaZ1ens7d+4cdqIe85jHDDto/RCpFiJaB/dlL3tZvOpVrxqGkT/6oz8adtxbh7wfrvPLv/zLww5a6wC223nnnRff+Z3fGdu3b99ve/7t3/4tnvWsZ8WJJ54YP/VTPxV3uctd4itf+Ur88z//8/Dn1oYrr7xyuN9f//Vf7/d4rePYOr4tVLzmNa8ZtrF1oNtQm49+9KPx8Ic/fI/9W2e/BZrXve51w85uC1JtHsrf/d3fDe+/5pprhq+ldYJf+9rXxpFHHjnc75/+6Z/22Y7Wht/7vd8btud+97vfcNvHP/7xYdWmfW3vZ7+tHyY2n4W8/r/5m7+JW265Zbhv65z/5m/+Zjz3uc+Niy66aI8hVePe+973DjvNz3/+82Mh2vO/5CUvGX5WfuM3fmM4lKkFova5aZ+J1pHvtQDR9mvvd/tMffjDH47f+Z3fGYafV7ziFcP3sz22fd/CXGtv00Llvj6XLTw8+9nPjo985CPx8pe/PB74wAfGBz/4wfj5n//5YZBp7/neXHXVVcPQ247bfpctHLVA1t6DUXf2dw6wpDqAZfIXf/EX7fRt9+EPf7i79tpru8suu6x75zvf2R1zzDHdpk2bussvv3y430te8pLhfq997Wv3ePzHP/7x4fZ3vOMde2z/wAc+sMf2a665ppuZmeme+cxndnNzc7v3+4Vf+IXhfu34vY985CPDbe1rs3Pnzu7000/vTjvttO6GG27Y43lGj/UTP/ETw8fNp21/3etet/vn5zznOcP2XHjhhbu3XXnlld1hhx3WPe5xj7vD+/PkJz95j+f66Z/+6W5ycrK78cYbhz+/613vGu732c9+tjsQ7X1pj3vb2942/Lkdb2Jiovue7/me7oQTTti936te9aru6KOP3t2Giy++ePi41r79vf5+3/Y7vf7663dvf8973jPc/r73vW+fbTzqqKO6s88+e0Gv55ZbbumOPPLI7od/+If32H7VVVd1RxxxxB7b+8/UG97whj32fdCDHtSdc845u39un8vx39/4McY/l+9+97uH29/0pjftsf35z39+NxgMum984xu7t7XP1ejn79WvfvXwsZ/+9Kf3+D219rft7f2s/M4BlpKhUMCye/KTnzw889qGjXzf933fcHjJu971rjj55JP32K+dOR7193//93HEEUfEU57ylLjuuut239rk3naMdsa4aWeiW2WiH+bTe/WrX73ftrWz3K0K0vZtZ4VHzTfcZ3/aWfI20bgNv7n73e++e3urhrzoRS8aTl6/+eab93jMj/zIj+zxXK3S0I7zzW9+c/hz365WQdmxY8eC29Le8za8qE1+blqFp81laGfW2/Cnr3/967srFu2M/J15vb3v/d7vHVZdRl9D0yoW+9LeizbEaCFaxaTNyWjzckY/D+01tapE/3kY9WM/9mN7/Nzatb82jRv/XLaJ7O05+4pPrw2NajmzDdXam/bYRzziEfGwhz1sj99TP8Sqd2d/5wBLyVAoYNm18fltmdm2clIbM9/Go4+vlNPua+PNR7WO70033RTHH3/8vMdtw0WavgN+5pln7nF/67CNdnb3NSyrHypU1eZGtOE57TWOu/e97z0cS3/ZZZcN54z0Tj311D3269vczyN5/OMfH8973vOG8wDaMJs2pKoFlxZU9jc5uHWkW2e2DxBtEn27HX300cOf2+/jC1/4wvBYFft7DXvThou1IVQL0Qehfo7OfMca1ebk9HMoRtu1vzbt73PZPm8nnXTSHQJR+/329+9Nu298KFwz/nmp/M4BlopgASy7dna2XxVqb1pnaTxstE54CxXveMc75n3MeKdxtWpnv+fTTwRulYR/+Id/GE7+ft/73jccz98m8bb5Am1bq97sTatEtFWV2ln6FiRa0GjHa9vbz62D3N7nvsKwVK9hb1pF5X/+53+GFaf9LQHcT3Bu8yzaPJhx40v+7q1NB2K+z+XBUPmdAywVwQJYtdok2zbM6dGPfvQdJreOX3ehP6M9OvyoVQ/2d3a6PUfTrqnRhmztzUKHCbWw0yb4tusXjPvqV7867KS2IWF3RhtC025tCdY2WboNn3nnO985XMJ3b/rA0IYRtVWa2kTgfqJ2m7jcgkV/Mbp9qQyT2pfv+q7viv/+7/+Of/zHf9zr0sPjv6sWNvf1uzoQd+Z1tc9b+1y2Ssto1aL9fvv79/XYvvIyar7Py539nQMsFXMsgFXrBS94wXCuwRvf+MY73NdW1emvyNw6mW3lobe85S17nCFvqyvtz4Mf/ODhMp5t3/ErPI8eq792wf6uAt3OkrfVfNo1OtoqPr02p6F1DFulYHzIzv60cDR+5r+tRNRs27Ztn49tr63NZWnDadpY/RbS+sDRhoG1s+Kt47q/C/wt9PUfqDYHos0/afMTvva1r8073K2t9tW01Znae/erv/qr8847GF+idyH61ccO5HW1Vcfa57KtTjaqvcctqLRlhvf12FZx+MxnPrNHu8ercpXfOcBSUbEAVq02zrwtX/prv/Zrw+EyrcPeAkQ749smdrfrTLRlSvvrE7T92rKxrfPWJmW3SbTtGgD70ioI7cx9O3PeOm5tWdvW0W1nn9syrW0IStOf0W8TdlsHtwWINhF9Pq0j3CoELUT8+I//+LDT3pabbR3Ctgzrgfqrv/qreNvb3jZcErWdtW9nytvwptbJbq91f1qIaGe573//+++e+9ACVQsLrTO/kPkVB/L6D0RrT5vI315He/9Hr7zdlgz+27/923jkIx85/Lm93va7evGLXzxsf3v+9ru/9NJL41/+5V+GoWm8s78/rRJ2n/vcZ7i0b5sH1OaetPk2+5pz0z4rbcnYX/zFXxyGx7PPPns4Yb+FybYIQF9ZmU9bfrgN5Xra0542XMq4X262VTK++MUvLtrvHGBJLOmaUwD70C+nur8lM9tynIcccshe73/7298+XCK0LVHblmy9//3v373mNa8ZLuHam52d7V7/+td3J5544nC/JzzhCd35559/h+U+x5eb7X3iE5/onvKUpwyP39rygAc8oHvLW96y+/62LO0rX/nK7rjjjhsuKTr6n9f5lis977zzuqc+9andoYce2m3evLl74hOf2P3Xf/3Xgt6f8Ta2Y73whS/sTj311G7Dhg3d8ccf3z3rWc/qzj333G4h3vrWtw6P94pXvGKP7W2Z27b93//93/fYPt9ys3t7/f2+v/Vbv3WH593bMq7zab/LtszuWWed1W3cuHH4nrXf+Zvf/ObupptuusP7097btkRr2/eMM87oXvrSl+7xfuztM9XaM/6nsf1e2nO1JYJH27yvz2Vb+ra196STTuqmp6e7M888c/gejC4b3Ix//povfvGL3eMf//hh208++eTujW98Y/fnf/7neyw3W/2dAyyFQfu/pYksAADAemGOBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAcPCuvD0YDOrPBgAArCoLveydigUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBcCIwWAwvAEAB0awABgxMTERk5OTw68AwMJNHcC+AGve7OzscjcBAFYlp+QAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAFgXJib8yQNYSv4rC8CaNxgMBAuAJTbouq5b0I6DwVK3BQAAWGEWGBdULAAAgDrBAmCJqfgCsB4IFgAHIVgIFwCsdYIFwBJrk4ZNHAZgrTN5GwAA2CuTtwEAgINGsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKpuqHADgwk5OTu7+fm5uLruuWtT0AQJ1gARx0s7Ozy90EAGCRGQoFAACUCRYAAECZYAEAAJQJFgAAQJnJ26wf5+TtzromIt69iO0BAFhDBAvWj8Mi4qTC4+cWsS0AAGuMYMH60y6ZcGV+XYjjI2JmidsEALDKCRasP+0SCn9+ABWIHylWOgAA1gGTtwEAgDLBAgAAKDMUivVtJid13zUiBjnv4rpcAWpHRJyyiPMrpiPirPzanufWiLgkh2b1Whs2RsSZGfvbfrflfq09p0fEoRExmfu3fTZExOURcdkitRMA4E4QLFjfjshQcXLExMRETGyYiLlD5mJu59yucHGf7Ogvxr+0ozIwzGWA2BIRN2WQ6bUQc2IGiD5wtGBxS0RcFRHHRMTRGU4iQ9ERGToECwBgGQkWrG+nZSf+gxGbu82x4Z4bYutxW2PL/bZEfDQiHhARm3LfwcjgwW5sVanJsW2DscnhmyPi7AwH/5sh47Tc9h8jj2tB4dER8cmsQmzOieMPjoj3R8S5I8dsz/GgiDgkIq5dwvcIAGABBAvWt5kMDpMRg9lBbLp2U8xePxuDmUF0j+12DTNqHfjm2Ih4cVYwvpa3KzJs/FQGgdbB37mrAhL/ZyRcbMwA07bdnvu0535yRHxkJFi0f5HHZfVhR95a+x4/MlSrN5GB40MZWAAAlpFgwfp2RVYFnh2x5bItsfWKrTF33Vx0W7qIb0XEQzMU9EOXPhcRN0bEY3IY0xV530xeG+PiiLg5Q8fcWAjYnMOa2vbtEbFtpBrSa8Ofbsi5HZdnNeKkefabye3fzrkaLYAAACwjwYL1rZ/fsC1ibmIu5k6Z29WZb6HimrEKQZcd/3b/4fPMvWjViuuzItECxKhBDpeaHTnW3Mgk7F573Jcj4tQcLjWXw6P6qkmvPfcZGWRuP4CL/QEALBHBgvVnkEOV+orCbFYbWmf+bjl5+sj81zExT9VhJm+j981lpeL2kWPGPKGkDxf9fI3x/bZnpeLeEXFCViO6sYrE5Mjci/PyMQAAy0ywYP1pHfOX72efVg143DzbWxD4Uq4WNV7N2FfVYC6rEZtzfkVb1WnDSBCJsbkYH80w0QeINtG7d0hWM7bmcKyFXkEcAGAJuUAeLFQLARdkMBmvZowajCwH29uaw5bulfMljsvKyEUZSPrjTef2MzJ4nJDVla+MBJdTMnx8RqgAAFYOFQvWj89lMBh1RFYE7pnViNZ5vzQvSNeGIT0lg8JtGQ6+Kydv9xe5my9ctPkXj4qID4yEgfb4/8mlZI/Px7Xjfz73eUREXB0R38wlZR+Sw6F25GTuL+d+h+Sci6mcBwIAsEIIFqwft+Rt1I15uyHP/nc5Afu6XLXpsxkstuf9R+bqUDdmx74fAvWJkWNvn+didTvzOb6WoaQfGtWOE7lc7M0ZJK7K/SayDTfnc/bHuSLbaG4FALCCDLquW9B6MoPB+LI0AADAWtctLC6YYwEAANQJFgAAQJlgAQAAlAkWAABAmWABAACUCRYAAECZYAEAAJQJFgAAQJlgAQAAlAkWAABAmWABAACUCRYAACzI5ORkzMzMxGAwWO6msAJNLXcDAABYHebm5qLruuENxgkWAAAsiFDBvhgKBQAAlAkWAABAmWABAACUCRYAAECZYAEAi8gynMB6JVgAwCISLID1SrAAgEVe5x9gPRIsAFbAGe6pKZcVAmB1EywAlpkLTgGwFggWACuA4TPAUpiY0NXj4PFpA1gBVCxg7VhJE/hXUltY+wQLAFjHnNFeXVpQOJDf2ezs7JK2B0b5rwkArFOtgzo9Pe2s9iqqQLZjGzrJSjXoFvjp9x8dAFh72t/3tTIUby29FlhJFvrvSsUCANaxtdQRX0uvBVYjwQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAFg1BoPBcjcB2AvBAgBYNbquW+4mAHshWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAKyA1a4mJnTLWN2mlrsBAADrXVvtyopXrHaiMQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAACyjqampmJycXO5mQNlU/RAAANxZc3Nz0XXdcjcDylQsAACWUQsVggVrgWABALCMhArWCsECAAAoEywAAIAywQIAACgTLAAAgDLBAoAVwTr+AKubYAHAirlI2MSEP0sAq9WgW+AaZ4PBYOlbAwAArMolkZ0aAgAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAYH0Fi4mJidi0aVMMBoPlbgoAALCag8XU1JRgAQAAK8yqChZd18Xc3NxyNwMAABgz6FpvfQFUCViTsXoxP9azi3gsAIAVYoFxIaaWvCWwUr0sIk5axOP9WUR8axGPBwCwiggWrF+TeQMAoEywgJsj4nMLGAo4iJgYTAwXEZicnBxumzt0Lrbdb9vBaikAwIolWEALFh/dFR76WwsP/dcWIvqv7dZWJpuenh4+dNtx2+La+1273K8AAGDZCRYwiGFYmJmZGQaG/vuNGzcOr5uyYcOG4c/9ra9WbN26Na7bcF1cG4IFwFrT/lvfVqJc6KRVQLCA2DCzIU4+9eQ47bTThqGiaX9IZmdnh+Fh27Ztceutt8b27duHP7db+37nzp2x4/gdEY9anD9g7Tn7GwDLp69Y+28yHBjBgnVv+5Hb44pnXhHXzly7e1nlLrr2f7vPVt3hazc3vL+bWpw/OLPPmI3B5YMYfGGw/z9ibcL5kRFxTkRsbOOxIuKiiLgkInaM7DcTEcdGxANzad2duc/F+ZgY2/fEiHhSRLw3Ir69KC8LYFVq/x1uJ4+ECjgwggXrXjfdxbZjtkX737I5JqK7YVeY2a8jc5ncloEu3fXYODrDQvu5d3xEnBIRt0bETRFxSEQcGhEnZxDpDfK+syLiLhGxa/oIwLomVMCBEyxYv66JiPELubcz/NsjYkte8G6QHe3pPKu/OTvwN0bEpuyQb8tKQZfbNo5cfO/WPN5cHuOIvK/tvzUibs/nbY/t8g/ZRIaFW/Jf6GQGg94xWV1o18z434g4MwPBKfMEixZA/jsiroyI4yLi7hFx2liw2JTBpN38HQUA7iTBgvXr3fNse1FEXBUR5+VqUYPskLdO+qk5rOiyiHh/RNwnhyNdnI/ZkZ38e2RnvQWCcyPi8oi4LUPLU9qkjojhfO/Wuf/i2PO3UHFYRPw/u1aqGoaIFkY+OLLPEVm1+K8MApdn+1q4GLUpg1BrW+TwpjPy9Yy6a7628zN0AADcCYIFy65NkGtWxCS5dj2L+0bECzNAfCnP9l8dEddFxL0j4u9yvsJEViO+nCHh0VmB+GQGh8Mj4slZwbgwQ8nHI+L6/P6+Y8FiMBI+/jOf/+J5LuI3k+GkVVUin7PL5xm1JQNNCzsXZHi4y9h+J+RzDnKfZyzR+woArHmCBcuuTYheMS7JM/tHZIf7ETm86KL/O1xp9/CpLodBXZbb7pJVglOyejGZw4suzQ7+uTmE6bgcotQCwqjTMqxclMfcns/RhmTNpxv7OpjntbRj3DPDxQ35nH37W/sekEOuvmAYFABQI1hAr59LsSXnRtyWnfWTc47DfCsltU5/P+d7Jjvpl4x03m/Ox81k4Ig8/taxYDGRFY7jstrRh4qYp8M/mxWTmdxvJts5uiJUHxzmsuoxke08POeFRD7X0RmGZnNIVPsvwul5/FahAQBYoF1jUIBdoeLkrFZsz8nR549M0o7swPeVgb6C0duZoeCCrABckJWHm/MYD8jO/dUZXEYrDH0waMc4Kp9vYi+ViG15a+2MXOlpkEFodP/D83ZhtueGbEs/EXxzBqHByETvfuJ4/3oBABZIxQJ6LSQ8JCKuyE76lgwEbQ7DtrFKwHzDk67Lf1H9cq6n5pyMC/O+o3JYVL/S02isn80g0m7fmY/7cj5vP5ejd1OGhDNyvsYpuU8/SXtDPm5zViVaO74+MjG7TfaObOPo6lCT+bznjhwLAGCBBAvotQDxiZyP8LysHrSw8emcwL0xvz53bJWm3meyY/6wiHhkPvYT2UmfzonaL8pQcFiGhfGQ0oLCv0TED2S4OTIrEqPPd2m29XE5f6KfQP7VDBjPiYgPZWhoQ64em+1pQ7K+EhHfOAjvJQCw7gy6BS7D01+RGNa06QwQ7ax//y/j1hymNBgZfnRTVjM2jMy9mMjHtlv/z+WWkccekvftHJn/0IJE5PCjrVklGb+OxUQOYRpv56F5X5dBY2t+f3S2by7nX/RDpXbOU30Zd0K+nrYvAEAs/IKRggUAAFAOFiZvAwAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggVLZjAYLHcTAAA4SAQLlkzXdcvdBAAADhLBAgAAKBMsAFizDMkEOHgECwDWpImJid03AJae/9oCsCZNTk7G9PS0YAFwkAy6Bc6wVU4GAID1p1vggjxO4wAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYALDbYDCIqamp5W4GAKuQYAHAvAEDAA6EYAHAbl3X7b4BwIEQLADYw+zs7HI3AYBVSLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAyqbqhwAAWNkGg8Hu77uuW9a2wFqlYgEArHlTU1OxYcOGmJycXO6mwJqlYgEArHlzc3N7fAUW36BbYD1wtIQIALDatL6MYVBw4Bb678ZQKABgXRAqYGkJFgAAQJlgAQAAlAkW61wbb9pWygAAgArBYp1r402tkAEAQJVggWABAECZYAEAAJQJFgAwj3aFZtdwAlg4wQIA5mGYKMCBsRwQAMzDxdQADoyKBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAwAoyGAyWuwlwpwgWAAAryMTEhHDBqjS13A0AAOD/mp2dXe4mwJ2iYgEAAJQJFgAAQJlgAQAAlAkWAABAmWABAACUCRbAQTM5OWkJRQBYowQL4KCYmpqKww8/PGZmZoQLAFiDBAvgoOi6Lnbs2BFzc3PL3RQAYAkMuvbXfiE7OsMILMJQqBYsFvifHQBgBVjo321X3gYOGleTBYC1y1AoAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoGyqfggAWIGnzV4WEZPF45wbEectUpsA1rhVFSwGg0F0XbfczQBgpRtExEmLECwOW6T2AKwDU6spVExM7Bq5NTs7u9zNAWC1uDlvBxpK2lcA1l6waKFiampXcwULABbscxHx0QPYv1U5fmERqh0A68yqCRYtTAgUAACwMlkVCgAAWD8VCwAoOzQi7hoRG3IOxVxEfD0itkbE4RGxMSKuW4TnacOojstb+0u7IyIuiYgtETG6BslMRBwfEcdke1ph/psRcWvu19pzZp4GbD/flsdpxwNYYQQLANaHQYaKU7JDPxExMTMRM1tnovtWFztP3BmzR80uTrBoq0mdnM81m39tt2SQaeGgd2zu08LFzgwkt+d+LTycGBGn5zEiH3tLRFy1CG0EWGSCBQDrJ1g8NiL+LSKujBhMDWLDvTbECc86YTi5+8aTbowbj75x17UrRh/TVwu6vWwbjN3f3DOrIq0a8uWIeFgGhLbvRSP7PSirE5/d1aa4W0TcI/e7ISIeHRGfjIjLI2Jzrlb14Ih4/9jzAawA5lgAsH4cEhHTWa3YPhGbr9gcj9r4qJi+z3TsOGVHxAkR8YKRv47nRMT3R8RDR5affVREvCIinpnbvzsi7jv2PHfNKsMlGQC+ktWJo8b2OzorFX0F4tLcdmSe+mtDqS6LiO0RcVOGjdMshQusTCoWAKwPrYPfqhEPiYj7RMxePRs3XXlT/Mc7/iNuvu3m2PagbbuGJX0oIu6ew5K+FhEXR8Qj8rFdbp/L7f2tDV8atSn37bdvyeFXLdSMujm33yUrFqflXI/pDCYtSJySFYtDsmLRjg2wAgkWAKwPXQ5N2p6TojdF7DxrZ1z7zWtj7uq56G7vds1ruH7kMTM54fuIsWO1Y1yRgeH2eYYl9RWPFkAiQ8JgnkrDhVmdOCPnWUyM/GW+LYdRnZqVjrmcu6FaAaxQggUAa1s/kbp3TQ43ahOj7x4xuzlnRh+e8yJaVaC3KUPF6MXyWojYkdWE0fAwqt82kd9PzjMPo7k6/xKfkO28KodGzWZ4aZWKe+f9/UpRVoQCVijBAoC17SF525tjckJ07+VjIaRNtj57ZNt8AWHc7VlZ2JRVjUMyKOyYZy5GCyj/m2HimBwOtTWrKqfnVcN3jEzebvcDrEAmbwPA3vRDk0YrFuOm5vlrenk+pl8JqlUdvp0hYjAy1+KYHOp0cm6/f1Ytrst9TsxhUhuyanFyTgS3IhSwAqlYALD2tLP/f7aX61jcOysJc7nfp3PFpdNzqdcuQ8BJOWRqY1Yb+knb4x6RQ5ra/I3eBRkI7jEyEfzzWQE5PFeW+kBEfCnvf1guPbstIr6azz8YmWx+75HhV23ehWABrECCBQBr07f2MkRpS1YDupHVnW7PTn0/cfvWXPp1Lm9XjYSKb+ZqTr2rxn6OvIjd5XnM/srb145M9G5LyEY+32S2aZCPuyH3H+Sxv5YVkW35PG1fgBVo0HXdgs57DAaWoQAAgPWmW1hcMMcCAACoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsEC1ripqanYuHFjDAaD5W4KALCGCRawxnVdF7Ozs8vdDABgjRt0rdexkB2d7YRVq/37XeA/dQCAPSy0D6FiAeuAUAEALDXBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAFj3BoPBcjcBVj3BAgAAKBMsAIB1r+u65W4CrHqCBQAAUCZYAAAAZYIFAABQJliw7lj5AwBg8QkWAABAmWDBumPlDwCAxSdYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAOvSYDBY7iYAsEpNTOhCz8e7AqzLUCFYAHBn+RsyP8ECWJe6rlvuJgCwSs3Ozi53E1akqeVuAMDBJlQAwOJTsQAAAMoECwAAoEywAAAAygQLAACgTLAAAADKBAsAAKBMsAAAAMoECwAAoEywAAAAygQLAACgbKp+CIC1Y3Jycvh1bm4uuq5b7uYAwKqhYgEAAJQJFgBjVCoA4MAZCgUwYnZ2drmbAACrkooFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFDmytsAANRMFh8/u0jtYFkJFgAA1ELF/1sMF38WEd9axDaxLAQL4KAbDAYxMbFrJObsrNNUAKve5CJULVj1BAvgoJuamhremttvv325mwPAYrk4Ii5Z4L6HRcRDlrg9HFSCBbAsFYt2A2CNaaHiowvc92TBYq0RLICDbvv27cMbALB2WG4WAAAoU7EAAGDxHRMR94uIQ9sY2FxS9hMRsSUijo+IMxfpeaYj4vS8zUTEtoj4XETcOLaM7eaIuHtEnJqn1ndExOcj4vqI2JnbHpbtbhPRb4qI8yLilkVq5zqgYgEAwOIaZKjYumsZ2cFVg5iO6Zi410TExog4Ljv5i+GkiDgqw8Hl2btt244Y2+/uua0Fhssi4raIODEfO52PaSHomoj4dh7nrHwtLIiKBQAAi6t1xu8VER+KiCsjJqYm4tA4NLaeujW2Xrk1uhO6iLuM7N8Hg64tF5hVjciO/iFZhWgVhg15/+iCgqfm810aEd+IiAdkYNia1YjeGRFxQ0R8JSKuzSBx/zz21gwet0bElzNotMnl98iqBQsiWAAAsPg25NCkqdZPn45jbzs2brn8lthx9x2x86Sde/ZCH5PDlrZnOPhUhowHRsQ5uYztVRFxSkRcEBHnjzz2hF3hZVitaI+5KEPNaKhoDo+I67Ia0VwdEY/N/TZm8HhftqEFjStySVwWTLAAAGBxtQ7+x7LjviNi6xVb48KLLoz4WsTc7NyufU7O8BE5J+LSrE48OyI+nceYyI7+lzMwfGFs3kSzKb+2MBBZdZjJqsOoGzJAnJLP1eZ4HJn7TubcijPz/skMHH07WBDBAgCAxdVlZeGa7LwfGzH38LmIozMkjHfWD80hTceMhI3+ONtyTsRchozxx/ZzIPrtewsC52VoODuHQF2bj+3y60QOwfpYBpzjMxj9s3CxUIIFAACLZ5A9zOkcenRrdth3ZkWgVR7GTeYcilvnmSw9m+Ei9tLBb4+LfL5+HkZ7TBZGdpvOFZ5uyxDRz+XYkcfdnsOjvp37TeV8jT58sF+CBQAAi+OwHOK0Mb/vl3KdzQrBo/L+I8bWJt2W+45Oyo7s0I926ufr5G/J7YfmcKcj8njbxh5zfK4IdVEOm7pr7n9btu+WvPVB5vax6gn7JVgAALA4HpK3fXnePNu+lZ36w7N6sbclXqezErFz7LGbcpjTLTkJ+/oMEYMMB1vz2JuzQnFZVk+uzX235wTwE/PrphzC1e5nwQQLAACW1zOzetBXCybnmaTdPClXfxpdFeqLEXHviLhvRDwoQ8THMhS0OR1PiYj/ExHn5j6Pz8BxdV4grw8gn4yIp0XEI/PnG3O53PEhVezVoOu6BY0aGwxcHQQAgHm0M/0Vs3nrsipxTX5/aFYc+iViD8+Kw+iQqUEOvdqU389l5WJnBpQjsioxkftszP36OR19gJkYG6K1c+z+daxbWFwQLAAAgHqwGJ02AwAAcKcIFgAAQJlgAQAAlAkWAABAmWABAACUCRYAAECZYAGwiCYnJy3PDcC6JFgALKK5OZdoBWB9mlruBsBCTU9PD88Et4u07NjRLpcJq/ciQgCw1ggWrKohJhMTEzE7O7vcTQEAYIyhUKy6s8HOCAMArDyDboG9NJMRAQBg/ekWeFJXxQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAom6ofAgBgiUwWHz+7SO0A9kuwAABWppMi4uWFx++MiN+IiLlFbBOwV4IFALAyDYoVi24R2wLsl2ABAKx850bELQvc9/SIuNsStwe4A8ECAFj5PhcR3zqASodgAQedVaEAAIAywQIAACgzFAoAWF09l6Mi4pz8vk3QvjkizouILRFxVkTcZZGeayaf56g8Fbs1Ij4eEdvHJoa31avuExGbRra3OSHXRcTmvP+BEXFr3te+Xh0RX1mkdsIKoWIBAKwerfN+34j4dsTMNTOx4bYNMbNpZlegaHMr7h4Rxy3Sc50WERszuFydK1TdLQNHrz3n/TN0tDkgV+xqW9wv2zqTX9v9l0fEZbnfjYvURlhBVCwAgNWjddLvFRHvjJianYqpo6Zi9oTZ2H7C9ojDI+LkiDhipNN/XFYRdmTn/va879i8f3te72Jjdvb7C+q1+1pYuSYiLs0Vqe4ZEfeIiKsiYtvIfm3bhzI4tOc5OiKeHxFfGOlpXZI/WwKXNUywAABW11iLVgWYidixZUd03+52dfpbReHs2BUu+vEY7evTI2I6Iq6NiIsi4ot53zOzF3RlVhjOiIgPRsT1I4HhrhFxfkTckOGjPf7hYxWLyLCSbRoGh01Z3Rjkc7ftN2V7upEbrDGCBQCwerRhSZ+MiBdF7Lh8R+y4dMeu4UWtqnBZDoU6JDvx7YrbH8vg8MAcQtUHi8mcA3F+Vhq+OFKF6B2S21pVI3IOx+axgeRzWa14Rt7XKhaHRsR7s62H5xyN4zKUtON9PSK+dgDX5YBVQrAAAFa+52anfSKHLR2WVYZTsuPf7muOGbta9+E57OmksUpDl1WEq/P7bfNUEVrFYdR8VYbWnodExAU5ROq2iDg129XmW0RWO9pzfSonlreQ8aAMPbCGCBYAwMo334TsTXnbm0H2dLbk/IrxYLFjpBoxX2jYno+fzMrEhnzM+L4nRsSXs/LRh5yn5TCom7KScktWSHZm0Dn+TrwHsMIJFgCsOhMTu8aidF03vLFGbc/O+mhQmMmhRdflROt+22G57dgMEdtyv9uzKnHSyKTu2Et1YnzbzRlcNubxjsiAMDrBOzJw3J77zOXjjsxe1tZ8TJunEVnRmM3QAWuMYAHAqrNp06ZhoNixY8fwxhrVJlz/2Vivpa369OyI+PvswG/I0PCoiPj/IuLFOcm63V6Qcy/6qsO+FtnfMDYcqsvHHp/h4Kacv3HZSCVjIisQN2SQuDUDxgnZttlsW1ui9uKcW3FMBhLLzbIGDboFnuoZDMYHGgLA8pie3nW6d3Z2Nubm2ili1o2N2VF/RFYHuuykfyYrE0/PTv723H6P7Pgfltv+Jjv8L4mIb+RE8IkMIR8aWRUqslrxuKyC9BfI+9esOtw/h0B9ML8+IudzdNmuj+RStdMZLh6TzzuXAeOCDCuwCiy0MixYALDq9H+TDINah/qhT23lpd7OnEexMzv3/RW5++tT7ByZJ9EHh2MyKLTHRV574qaRYU79cx2Wxxvk428cWVJ2OisTU9me0UnjbfuOedrbTxRvlQ2ZmFVCsAAAAA5asNjXaEMAAIAFESwAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECANYpS8kDi0mwAIB1ygUGgcUkWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJlgAAABlggUAAFAmWAAAAGWCBQAAUCZYAAAAZYIFAABQJljAOjYYDJa7CWua9xeA9USwgHWs67rlbsKaDRRTU1OxYcOG5W4KABw0ggXAEpibm4vZ2dnlbgYAHDRTB++pANZPJai/AcB6oWIBsEQECwDWE8ECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAsqmF7th1Xf3ZAACANUnFAgAAKBMsAACAMsECAAAoEywAAIAywQIAACgTLAAAgDLBAgAAKBMsAACAMsECAACIqv8fWdNsUC+Q3i0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define label map\n",
    "label_map = {1: 'Star', 2: 'Streak'}\n",
    "\n",
    "# Transformation\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image), image\n",
    "\n",
    "def get_centroids(boxes):\n",
    "    \"\"\"Compute centroids from bounding boxes.\"\"\"\n",
    "    centroids = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        centroids.append((cx.item(), cy.item()))\n",
    "    return centroids\n",
    "\n",
    "def predict_centroids(model, image_path, device, threshold=0.5, visualize=True):\n",
    "    model.eval()\n",
    "    image_tensor, orig_image = load_image(image_path)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor])[0]\n",
    "\n",
    "    boxes = prediction['boxes']\n",
    "    labels = prediction['labels']\n",
    "    scores = prediction['scores']\n",
    "\n",
    "    # Filter predictions\n",
    "    keep = scores >= threshold\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "\n",
    "    # Calculate centroids\n",
    "    centroids = get_centroids(boxes)\n",
    "\n",
    "    # Print centroids with labels\n",
    "    print(\"Detected Objects with Centroid Coordinates:\")\n",
    "    for i, (label, score, centroid) in enumerate(zip(labels, scores, centroids)):\n",
    "        label_str = label_map.get(label.item(), f\"Class {label.item()}\")\n",
    "        print(f\"{label_str} (Score: {score.item():.2f}) -> Centroid: (x={centroid[0]:.2f}, y={centroid[1]:.2f})\")\n",
    "\n",
    "    # Optional: Visualize\n",
    "    if visualize:\n",
    "        label_texts = [f\"{label_map[l.item()]}: {s.item():.2f}\" for l, s in zip(labels, scores)]\n",
    "        drawn_image = draw_bounding_boxes(\n",
    "            (image_tensor * 255).byte().cpu(),\n",
    "            boxes=boxes.cpu(),\n",
    "            labels=label_texts,\n",
    "            colors=\"green\",\n",
    "            width=2,\n",
    "            font_size=20\n",
    "        )\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(drawn_image.permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Predictions with Centroids\")\n",
    "        plt.show()\n",
    "\n",
    "    return centroids\n",
    "\n",
    "# Usage\n",
    "image_path = \"Datasets/train_images/Raw_Observation_001_Set1_enhanced_plot.png\"\n",
    "model.load_state_dict(torch.load(\"model_output/model_checkpoint.pth\", map_location=device))\n",
    "centroids = predict_centroids(model, image_path, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detection_accuracy(model, data_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"Evaluating Accuracy\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        for pred, target in zip(outputs, targets):\n",
    "            pred_boxes = pred['boxes']\n",
    "            pred_labels = pred['labels']\n",
    "            pred_scores = pred['scores']\n",
    "\n",
    "            true_boxes = target['boxes']\n",
    "            true_labels = target['labels']\n",
    "\n",
    "            if len(pred_boxes) == 0 or len(true_boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            ious = box_iou(pred_boxes, true_boxes)\n",
    "\n",
    "            for i, (box, label) in enumerate(zip(pred_boxes, pred_labels)):\n",
    "                max_iou, max_idx = ious[i].max(0)\n",
    "                if max_iou >= iou_threshold and label == true_labels[max_idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"Detection Accuracy: {accuracy*100:.2f}% ({correct}/{total})\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c55631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|██████████| 28/28 [00:58<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Accuracy: 59.70% (197/330)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|██████████| 7/7 [00:12<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Accuracy: 75.00% (42/56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_acc = compute_detection_accuracy(model, train_loader, device)\n",
    "test_acc = compute_detection_accuracy(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c87b3358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-3): 4 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87402c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
